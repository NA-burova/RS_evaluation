{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets\n",
        "!pip install -q kaggle\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "data_load='/content/data/'\n",
        "data_save='/content/data/drive/MyDrive/RS/Weather/'\n",
        "\n",
        "import torch, torch.nn as nn, torch.nn.functional as F\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import shutil\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "from torchvision.models import regnet_y_32gf, RegNet_Y_32GF_Weights, efficientnet_v2_l, EfficientNet_V2_L_Weights, vit_b_16, ViT_B_16_Weights, convnext_base, ConvNeXt_Base_Weights\n",
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d jehanbhathena/weather-dataset"
      ],
      "metadata": {
        "id": "joOEZw0lxKuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip weather-dataset -d data"
      ],
      "metadata": {
        "id": "iJWTyX82ApmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "data_path=data_load+'/dataset'\n",
        "data_new=data_load+'/weather_all'\n",
        "!mkdir data_load+'/weather_all'\n",
        "labels=[]\n",
        "for i in os.listdir(data_path):\n",
        "  for j in os.listdir(data_path+'/'+i):\n",
        "    labels.append([j, i])\n",
        "    shutil.move(data_path+'/'+i+'/'+j, data_new+'/'+j)\n",
        "data=pd.DataFrame(labels, columns=['file', 'label'])\n",
        "data"
      ],
      "metadata": {
        "id": "xENVnuHIAv0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_matcher={}\n",
        "for i, j in enumerate(np.unique(data['label'])):\n",
        "  data['label'][data['label']==j]=i\n",
        "  label_matcher[i]=j\n",
        "data"
      ],
      "metadata": {
        "id": "zn33mdg1AvU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train, data_test = train_test_split(data, test_size=0.1, random_state=42)\n",
        "data_train, data_val = train_test_split(data_train, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "pcxbkbI-Ecsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(data_train), len(data_test), len(data_val)"
      ],
      "metadata": {
        "id": "G7JxklUbyqCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DS(Dataset):\n",
        "    def __init__(self, labels, root_dir, subset=False, transform=None):\n",
        "        self.labels = labels\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.labels.iloc[idx, 0]\n",
        "        fullname = os.path.join(self.root_dir, img_name)\n",
        "        image = Image.open(fullname).convert('RGB')\n",
        "        labels = self.labels.iloc[idx, 1]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return [image, labels]"
      ],
      "metadata": {
        "id": "wSQ-nY7nElPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(dataloaders,\n",
        "                model,\n",
        "                criterion,\n",
        "                optimizer,\n",
        "                num_epochs=1,\n",
        "                file_name='model.pth'):\n",
        "    train_losses = np.zeros((num_epochs))\n",
        "    test_losses = np.zeros((num_epochs))\n",
        "    k=0\n",
        "    best_loss=np.inf\n",
        "    for epoch in range(num_epochs):\n",
        "      loss_avg = 0\n",
        "      for x, y in dataloaders['train']:\n",
        "          logits = model(x.to(device)).cpu()\n",
        "          loss = criterion(logits, y)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          loss_avg += loss.item()\n",
        "          optimizer.zero_grad()\n",
        "      loss_avg = loss_avg / len(dataloaders['train'])\n",
        "\n",
        "      train_losses[epoch] = loss_avg\n",
        "\n",
        "      loss_avg = 0\n",
        "      for x, y in dataloaders['valid']:\n",
        "          logits = model(x.to(device)).cpu()\n",
        "          loss = criterion(logits, y)\n",
        "          loss_avg += loss.item()\n",
        "      loss_avg = loss_avg / len(dataloaders['valid'])\n",
        "      if loss_avg<best_loss: #early stop\n",
        "        best_loss=loss_avg\n",
        "        k=0\n",
        "        torch.save(model.state_dict(), file_name) #saving model with best loss\n",
        "      else: k+=1\n",
        "      if k==20:\n",
        "        print(\"No quality improvement in 20 last steps. Training stopped earlier\")\n",
        "        break\n",
        "\n",
        "      test_losses[epoch] = loss_avg\n",
        "      print(\"Epoch %d/%d: Train loss = %.4f - Validation loss = %.4f\" \n",
        "            % (epoch + 1, num_epochs, train_losses[epoch], test_losses[epoch]))\n",
        "    \n",
        "    return model"
      ],
      "metadata": {
        "id": "_WiglyS2HDXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights=RegNet_Y_32GF_Weights.DEFAULT\n",
        "preprocess = weights.transforms()\n",
        "regnet = regnet_y_32gf(weights=weights)\n",
        "\n",
        "train_ds = DS(data_train, data_new, transform=preprocess)\n",
        "valid_ds = DS(data_val, data_new , transform=preprocess)\n",
        "test_ds = DS(data_test, data_new, transform=preprocess)\n",
        "file=data_save+'regnet.pth'\n",
        "\n",
        "nw=2\n",
        "bs=100\n",
        "train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True, num_workers=nw)\n",
        "valid_dl = DataLoader(valid_ds, batch_size=bs, shuffle=True, num_workers=nw)\n",
        "test_dl = DataLoader(test_ds, batch_size=bs, shuffle=True, num_workers=nw)\n",
        "\n",
        "for param in regnet.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "xYkgdywwHMj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dloaders = {'train':train_dl, 'valid':valid_dl}"
      ],
      "metadata": {
        "id": "DIEKFKmQHbvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regnet.fc = nn.Linear(regnet.fc.in_features, 11)\n",
        "regnet = regnet.to(device)"
      ],
      "metadata": {
        "id": "ItSH9okQHQcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(regnet.parameters(), lr=1e-4)\n",
        "regnet_tuned=train_model(dloaders, regnet, criterion, optimizer, 200, file)"
      ],
      "metadata": {
        "id": "O8zAIV_5HVIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regnet_tuned.load_state_dict(torch.load(file))"
      ],
      "metadata": {
        "id": "NyyrLMYuHjn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred=[]\n",
        "y_true=[]\n",
        "for x, y in test_dl:\n",
        "    logits =  regnet_tuned(x.to(device)).cpu()\n",
        "    y_pred.append(logits.max(1)[1].data)\n",
        "    y_true.append(y.data)\n",
        "y_pred=list(torch.cat(y_pred, dim=0).numpy())\n",
        "y_true=list(torch.cat(y_true, dim=0).numpy())\n",
        "print(classification_report(y_true, y_pred))"
      ],
      "metadata": {
        "id": "fbqI8sCAHwCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights=EfficientNet_V2_L_Weights.DEFAULT\n",
        "preprocess = weights.transforms()\n",
        "enet = efficientnet_v2_l(weights=weights)\n",
        "\n",
        "train_ds = DS(data_train, data_new, transform=preprocess)\n",
        "valid_ds = DS(data_val, data_new , transform=preprocess)\n",
        "test_ds = DS(data_test, data_new, transform=preprocess)\n",
        "file=data_save+'enet.pth'\n",
        "\n",
        "nw=2\n",
        "bs=100\n",
        "train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True, num_workers=nw)\n",
        "valid_dl = DataLoader(valid_ds, batch_size=bs, shuffle=True, num_workers=nw)\n",
        "test_dl = DataLoader(test_ds, batch_size=bs, shuffle=True, num_workers=nw)\n",
        "\n",
        "for param in enet.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "xMWMidO6QArR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dloaders = {'train':train_dl, 'valid':valid_dl}"
      ],
      "metadata": {
        "id": "kEvEECmfQArY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enet.classifier[1] = nn.Linear(enet.classifier[1].in_features, 11)\n",
        "enet = enet.to(device)"
      ],
      "metadata": {
        "id": "EkG7COGKQArd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(enet.parameters(), lr=1e-4)\n",
        "enet_tuned=train_model(dloaders, enet, criterion, optimizer, 200-15, file)"
      ],
      "metadata": {
        "id": "LSu78L1bQAre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enet_tuned.load_state_dict(torch.load(file))"
      ],
      "metadata": {
        "id": "ghR9epRcQArg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred=[]\n",
        "y_true=[]\n",
        "for x, y in test_dl:\n",
        "    logits =  enet_tuned(x.to(device)).cpu()\n",
        "    y_pred.append(logits.max(1)[1].data)\n",
        "    y_true.append(y.data)\n",
        "y_pred=list(torch.cat(y_pred, dim=0).numpy())\n",
        "y_true=list(torch.cat(y_true, dim=0).numpy())\n",
        "print(classification_report(y_true, y_pred))"
      ],
      "metadata": {
        "id": "IoOyHt8RQArj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights=ViT_B_16_Weights.DEFAULT\n",
        "preprocess = weights.transforms()\n",
        "vit = vit_b_16(weights=weights)\n",
        "\n",
        "train_ds = DS(data_train, data_new, transform=preprocess)\n",
        "valid_ds = DS(data_val, data_new , transform=preprocess)\n",
        "test_ds = DS(data_test, data_new, transform=preprocess)\n",
        "file=data_save+'vit.pth'\n",
        "\n",
        "nw=2\n",
        "bs=100\n",
        "train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True, num_workers=nw)\n",
        "valid_dl = DataLoader(valid_ds, batch_size=bs, shuffle=True, num_workers=nw)\n",
        "test_dl = DataLoader(test_ds, batch_size=bs, shuffle=True, num_workers=nw)\n",
        "\n",
        "for param in vit.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "IEOo-N9ZQCCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dloaders = {'train':train_dl, 'valid':valid_dl}"
      ],
      "metadata": {
        "id": "-HA2iKJWQCCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vit.heads.head = nn.Linear(vit.heads.head.in_features, 11)\n",
        "vit = vit.to(device)"
      ],
      "metadata": {
        "id": "4I76_rptQCCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(vit.parameters(), lr=1e-4)\n",
        "vit_tuned=train_model(dloaders, vit, criterion, optimizer, 200-38, file)"
      ],
      "metadata": {
        "id": "Q5tkK7InQCCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vit_tuned.load_state_dict(torch.load(file))"
      ],
      "metadata": {
        "id": "NLPY7p8dQCCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred=[]\n",
        "y_true=[]\n",
        "for x, y in test_dl:\n",
        "    logits =  vit_tuned(x.to(device)).cpu()\n",
        "    y_pred.append(logits.max(1)[1].data)\n",
        "    y_true.append(y.data)\n",
        "y_pred=list(torch.cat(y_pred, dim=0).numpy())\n",
        "y_true=list(torch.cat(y_true, dim=0).numpy())\n",
        "print(classification_report(y_true, y_pred))"
      ],
      "metadata": {
        "id": "0ZUHAj4FQCCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights=ConvNeXt_Base_Weights.DEFAULT\n",
        "preprocess = weights.transforms()\n",
        "convnext = convnext_base(weights=weights)\n",
        "\n",
        "train_ds = DS(data_train, data_new, transform=preprocess)\n",
        "valid_ds = DS(data_val, data_new , transform=preprocess)\n",
        "test_ds = DS(data_test, data_new, transform=preprocess)\n",
        "file=data_save+'convnext.pth'\n",
        "\n",
        "nw=2\n",
        "bs=100\n",
        "train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True, num_workers=nw)\n",
        "valid_dl = DataLoader(valid_ds, batch_size=bs, shuffle=True, num_workers=nw)\n",
        "test_dl = DataLoader(test_ds, batch_size=bs, shuffle=True, num_workers=nw)\n",
        "\n",
        "for param in convnext.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "Alr0_h9hQDZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dloaders = {'train':train_dl, 'valid':valid_dl}"
      ],
      "metadata": {
        "id": "mIshUl1oQDZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "convnext.classifier[2] = nn.Linear(convnext.classifier[2].in_features, 11)\n",
        "convnext = convnext.to(device)"
      ],
      "metadata": {
        "id": "FTy8E6l5QDZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(convnext.parameters(), lr=1e-4)\n",
        "convnext_tuned=train_model(dloaders, convnext, criterion, optimizer, 200, file)"
      ],
      "metadata": {
        "id": "b2riTPw-QDZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "convnext_tuned.load_state_dict(torch.load(file))"
      ],
      "metadata": {
        "id": "5AsFO8H0QDZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred=[]\n",
        "y_true=[]\n",
        "for x, y in test_dl:\n",
        "    logits =  convnext_tuned(x.to(device)).cpu()\n",
        "    y_pred.append(logits.max(1)[1].data)\n",
        "    y_true.append(y.data)\n",
        "y_pred=list(torch.cat(y_pred, dim=0).numpy())\n",
        "y_true=list(torch.cat(y_true, dim=0).numpy())\n",
        "print(classification_report(y_true, y_pred))"
      ],
      "metadata": {
        "id": "S0XZrQgeQDZ-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}